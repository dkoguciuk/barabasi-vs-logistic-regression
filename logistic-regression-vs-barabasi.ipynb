{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "\n",
    "lr_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('data', train=True, download=True,\n",
    "                                                                      transform=torchvision.transforms.Compose([\n",
    "                                                                          torchvision.transforms.ToTensor(),\n",
    "                                                                          torchvision.transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                                           batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('data', train=False, download=True,\n",
    "                                                                     transform=torchvision.transforms.Compose([\n",
    "                                                                         torchvision.transforms.ToTensor(),\n",
    "                                                                         torchvision.transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                                          batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At birth accuracy: 11 %\n",
      "After epoch: 0. Loss: 0.3856881856918335. Accuracy: 90%.\n",
      "After epoch: 1. Loss: 0.3124147653579712. Accuracy: 91%.\n",
      "After epoch: 2. Loss: 0.1901261806488037. Accuracy: 91%.\n",
      "After epoch: 3. Loss: 0.1867780089378357. Accuracy: 91%.\n",
      "After epoch: 4. Loss: 0.2631455659866333. Accuracy: 91%.\n",
      "After epoch: 5. Loss: 0.5071932673454285. Accuracy: 92%.\n",
      "After epoch: 6. Loss: 0.14333565533161163. Accuracy: 91%.\n",
      "Logistic regression model acc: 92 %\n"
     ]
    }
   ],
   "source": [
    "def learn(model):\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate)\n",
    "\n",
    "    # calculate Accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total+= labels.size(0)\n",
    "        correct+= (predicted == labels).sum()\n",
    "    accuracy = 100 * correct/total\n",
    "    print(\"At birth accuracy: {} %\".format(accuracy.item()))\n",
    "    \n",
    "    last_acc = 0.\n",
    "    for epoch in range(int(epochs)):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = Variable(images.view(-1, 28 * 28))\n",
    "            labels = Variable(labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # calculate Accuracy\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = Variable(images.view(-1, 28*28))\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total+= labels.size(0)\n",
    "            correct+= (predicted == labels).sum()\n",
    "        accuracy = 100 * correct/total\n",
    "        print(\"After epoch: {}. Loss: {}. Accuracy: {}%.\".format(epoch, loss.item(), accuracy))\n",
    "        \n",
    "        if last_acc > accuracy:\n",
    "            return last_acc.item()\n",
    "        else:\n",
    "            last_acc = accuracy\n",
    "    return last_acc.item()\n",
    "            \n",
    "            \n",
    "model = LogisticRegression(input_dim, output_dim)\n",
    "logistic_acc = learn(model)\n",
    "print('Logistic regression model acc: {} %'.format(logistic_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Barabasi model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BarabasiRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, genes_dim, output_dim):\n",
    "        super(BarabasiRegression, self).__init__()\n",
    "        self.xi = torch.nn.Linear(input_dim, genes_dim, bias=False)\n",
    "        self.o = torch.nn.Linear(genes_dim, genes_dim, bias=False)\n",
    "        self.xo = torch.nn.Linear(genes_dim, output_dim, bias=True)\n",
    "        self.xo.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.xo(self.o(self.xi(x)))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At birth accuracy: 7 %\n",
      "After epoch: 0. Loss: 1.7259771823883057. Accuracy: 27%.\n",
      "After epoch: 1. Loss: 1.6338917016983032. Accuracy: 30%.\n",
      "After epoch: 2. Loss: 1.5750901699066162. Accuracy: 32%.\n",
      "After epoch: 3. Loss: 1.5732593536376953. Accuracy: 33%.\n",
      "After epoch: 4. Loss: 1.564858317375183. Accuracy: 34%.\n",
      "After epoch: 5. Loss: 1.527112364768982. Accuracy: 34%.\n",
      "After epoch: 6. Loss: 1.8827240467071533. Accuracy: 36%.\n",
      "After epoch: 7. Loss: 1.5211623907089233. Accuracy: 36%.\n",
      "After epoch: 8. Loss: 1.5618678331375122. Accuracy: 37%.\n",
      "After epoch: 9. Loss: 1.753145456314087. Accuracy: 38%.\n",
      "After epoch: 10. Loss: 1.55521821975708. Accuracy: 38%.\n",
      "After epoch: 11. Loss: 1.619829535484314. Accuracy: 37%.\n",
      "Barabasi model with 1 genes acc: 38 %\n"
     ]
    }
   ],
   "source": [
    "model_b1 = BarabasiRegression(input_dim, 1, output_dim)\n",
    "barabasi_1 = learn(model_b1)\n",
    "print('Barabasi model with {} genes acc: {} %'.format(1, barabasi_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At birth accuracy: 10 %\n",
      "After epoch: 0. Loss: 1.8842389583587646. Accuracy: 26%.\n",
      "After epoch: 1. Loss: 1.8064672946929932. Accuracy: 23%.\n",
      "At birth accuracy: 11 %\n",
      "After epoch: 0. Loss: 1.4831790924072266. Accuracy: 46%.\n",
      "After epoch: 1. Loss: 1.2193195819854736. Accuracy: 53%.\n",
      "After epoch: 2. Loss: 1.0885307788848877. Accuracy: 57%.\n",
      "After epoch: 3. Loss: 1.0486156940460205. Accuracy: 58%.\n",
      "After epoch: 4. Loss: 1.279468297958374. Accuracy: 60%.\n",
      "After epoch: 5. Loss: 0.9607208967208862. Accuracy: 61%.\n",
      "After epoch: 6. Loss: 1.3377606868743896. Accuracy: 60%.\n",
      "At birth accuracy: 10 %\n",
      "After epoch: 0. Loss: 1.1107027530670166. Accuracy: 70%.\n",
      "After epoch: 1. Loss: 1.0123831033706665. Accuracy: 75%.\n",
      "After epoch: 2. Loss: 0.5513543486595154. Accuracy: 76%.\n",
      "After epoch: 3. Loss: 0.6114587187767029. Accuracy: 77%.\n",
      "After epoch: 4. Loss: 0.8947972059249878. Accuracy: 77%.\n",
      "After epoch: 5. Loss: 0.8547793030738831. Accuracy: 77%.\n",
      "After epoch: 6. Loss: 0.628029465675354. Accuracy: 77%.\n",
      "After epoch: 7. Loss: 0.5241570472717285. Accuracy: 77%.\n",
      "After epoch: 8. Loss: 0.8954342007637024. Accuracy: 77%.\n",
      "After epoch: 9. Loss: 0.6098434925079346. Accuracy: 78%.\n",
      "After epoch: 10. Loss: 1.1903846263885498. Accuracy: 78%.\n",
      "After epoch: 11. Loss: 0.47116297483444214. Accuracy: 78%.\n",
      "After epoch: 12. Loss: 0.9777640104293823. Accuracy: 78%.\n",
      "After epoch: 13. Loss: 0.7515429258346558. Accuracy: 78%.\n",
      "After epoch: 14. Loss: 0.7661060094833374. Accuracy: 78%.\n",
      "After epoch: 15. Loss: 0.46004122495651245. Accuracy: 78%.\n",
      "After epoch: 16. Loss: 0.7926708459854126. Accuracy: 78%.\n",
      "After epoch: 17. Loss: 0.9148175716400146. Accuracy: 78%.\n",
      "After epoch: 18. Loss: 0.48650315403938293. Accuracy: 79%.\n",
      "After epoch: 19. Loss: 0.7188814878463745. Accuracy: 79%.\n",
      "At birth accuracy: 14 %\n",
      "After epoch: 0. Loss: 0.7347491979598999. Accuracy: 78%.\n",
      "After epoch: 1. Loss: 0.6411325335502625. Accuracy: 82%.\n",
      "After epoch: 2. Loss: 0.6089397668838501. Accuracy: 84%.\n",
      "After epoch: 3. Loss: 0.26398107409477234. Accuracy: 84%.\n",
      "After epoch: 4. Loss: 0.7246513366699219. Accuracy: 85%.\n",
      "After epoch: 5. Loss: 0.3399272859096527. Accuracy: 85%.\n",
      "After epoch: 6. Loss: 0.1920398324728012. Accuracy: 85%.\n",
      "After epoch: 7. Loss: 0.27241000533103943. Accuracy: 86%.\n",
      "After epoch: 8. Loss: 0.4430869519710541. Accuracy: 86%.\n",
      "After epoch: 9. Loss: 0.915005087852478. Accuracy: 86%.\n",
      "After epoch: 10. Loss: 0.5562729239463806. Accuracy: 86%.\n",
      "After epoch: 11. Loss: 0.4852483868598938. Accuracy: 87%.\n",
      "After epoch: 12. Loss: 0.3187352418899536. Accuracy: 87%.\n",
      "After epoch: 13. Loss: 0.5695747137069702. Accuracy: 86%.\n",
      "At birth accuracy: 14 %\n",
      "After epoch: 0. Loss: 0.6458178758621216. Accuracy: 77%.\n",
      "After epoch: 1. Loss: 0.5963268280029297. Accuracy: 84%.\n",
      "After epoch: 2. Loss: 0.6181778907775879. Accuracy: 86%.\n",
      "After epoch: 3. Loss: 0.9744527339935303. Accuracy: 88%.\n",
      "After epoch: 4. Loss: 0.39631032943725586. Accuracy: 88%.\n",
      "After epoch: 5. Loss: 0.6430917382240295. Accuracy: 88%.\n",
      "After epoch: 6. Loss: 0.3693977892398834. Accuracy: 88%.\n",
      "After epoch: 7. Loss: 0.3800899088382721. Accuracy: 88%.\n",
      "After epoch: 8. Loss: 0.23850783705711365. Accuracy: 88%.\n",
      "After epoch: 9. Loss: 0.5996165871620178. Accuracy: 89%.\n",
      "After epoch: 10. Loss: 0.3520604968070984. Accuracy: 88%.\n",
      "At birth accuracy: 10 %\n",
      "After epoch: 0. Loss: 0.715508222579956. Accuracy: 78%.\n",
      "After epoch: 1. Loss: 0.39973214268684387. Accuracy: 84%.\n",
      "After epoch: 2. Loss: 0.2412126362323761. Accuracy: 87%.\n",
      "After epoch: 3. Loss: 0.1958840787410736. Accuracy: 88%.\n",
      "After epoch: 4. Loss: 0.3067720830440521. Accuracy: 89%.\n",
      "After epoch: 5. Loss: 0.20410531759262085. Accuracy: 89%.\n",
      "After epoch: 6. Loss: 0.44116339087486267. Accuracy: 89%.\n",
      "After epoch: 7. Loss: 0.4239555597305298. Accuracy: 90%.\n",
      "After epoch: 8. Loss: 0.20881959795951843. Accuracy: 90%.\n",
      "After epoch: 9. Loss: 0.2406914383172989. Accuracy: 90%.\n",
      "After epoch: 10. Loss: 0.11729509383440018. Accuracy: 90%.\n",
      "After epoch: 11. Loss: 0.12040133774280548. Accuracy: 90%.\n",
      "After epoch: 12. Loss: 0.11549440771341324. Accuracy: 90%.\n",
      "After epoch: 13. Loss: 0.470499724149704. Accuracy: 90%.\n",
      "After epoch: 14. Loss: 0.5640950798988342. Accuracy: 90%.\n",
      "After epoch: 15. Loss: 0.20462511479854584. Accuracy: 90%.\n",
      "After epoch: 16. Loss: 0.15932825207710266. Accuracy: 90%.\n",
      "After epoch: 17. Loss: 0.3917306959629059. Accuracy: 90%.\n",
      "After epoch: 18. Loss: 0.34664830565452576. Accuracy: 90%.\n",
      "After epoch: 19. Loss: 0.26299190521240234. Accuracy: 90%.\n",
      "At birth accuracy: 8 %\n",
      "After epoch: 0. Loss: 0.8523194193840027. Accuracy: 86%.\n",
      "After epoch: 1. Loss: 0.3701082170009613. Accuracy: 88%.\n",
      "After epoch: 2. Loss: 0.2712143659591675. Accuracy: 89%.\n",
      "After epoch: 3. Loss: 0.5542031526565552. Accuracy: 89%.\n",
      "After epoch: 4. Loss: 0.3097103238105774. Accuracy: 90%.\n",
      "After epoch: 5. Loss: 0.24757462739944458. Accuracy: 90%.\n",
      "After epoch: 6. Loss: 0.39907392859458923. Accuracy: 90%.\n",
      "After epoch: 7. Loss: 0.9588709473609924. Accuracy: 90%.\n",
      "After epoch: 8. Loss: 0.3239728808403015. Accuracy: 90%.\n",
      "After epoch: 9. Loss: 0.44456514716148376. Accuracy: 91%.\n",
      "After epoch: 10. Loss: 0.1273045539855957. Accuracy: 91%.\n",
      "After epoch: 11. Loss: 0.33687421679496765. Accuracy: 91%.\n",
      "After epoch: 12. Loss: 0.38740015029907227. Accuracy: 91%.\n",
      "After epoch: 13. Loss: 0.14626219868659973. Accuracy: 91%.\n",
      "After epoch: 14. Loss: 0.45633745193481445. Accuracy: 91%.\n",
      "After epoch: 15. Loss: 0.3485507071018219. Accuracy: 91%.\n",
      "After epoch: 16. Loss: 0.4335780739784241. Accuracy: 91%.\n",
      "After epoch: 17. Loss: 0.3255402743816376. Accuracy: 91%.\n",
      "After epoch: 18. Loss: 0.16228832304477692. Accuracy: 91%.\n",
      "After epoch: 19. Loss: 0.5435892939567566. Accuracy: 91%.\n",
      "At birth accuracy: 9 %\n",
      "After epoch: 0. Loss: 0.4839610457420349. Accuracy: 87%.\n",
      "After epoch: 1. Loss: 0.3393590748310089. Accuracy: 89%.\n",
      "After epoch: 2. Loss: 0.4340746998786926. Accuracy: 90%.\n",
      "After epoch: 3. Loss: 0.44062498211860657. Accuracy: 90%.\n",
      "After epoch: 4. Loss: 0.4484451711177826. Accuracy: 91%.\n",
      "After epoch: 5. Loss: 0.11514512449502945. Accuracy: 91%.\n",
      "After epoch: 6. Loss: 0.12334276735782623. Accuracy: 91%.\n",
      "After epoch: 7. Loss: 0.32244011759757996. Accuracy: 91%.\n",
      "After epoch: 8. Loss: 0.21827679872512817. Accuracy: 91%.\n",
      "After epoch: 9. Loss: 0.2717902958393097. Accuracy: 91%.\n",
      "After epoch: 10. Loss: 0.08376523852348328. Accuracy: 91%.\n",
      "After epoch: 11. Loss: 0.07329194247722626. Accuracy: 91%.\n",
      "After epoch: 12. Loss: 0.22943775355815887. Accuracy: 91%.\n",
      "After epoch: 13. Loss: 0.22337870299816132. Accuracy: 91%.\n",
      "After epoch: 14. Loss: 0.20522885024547577. Accuracy: 92%.\n",
      "After epoch: 15. Loss: 0.1435522884130478. Accuracy: 92%.\n",
      "After epoch: 16. Loss: 0.07926443219184875. Accuracy: 92%.\n",
      "After epoch: 17. Loss: 0.35777461528778076. Accuracy: 92%.\n",
      "After epoch: 18. Loss: 0.2217569798231125. Accuracy: 92%.\n",
      "After epoch: 19. Loss: 0.16796648502349854. Accuracy: 92%.\n"
     ]
    }
   ],
   "source": [
    "barabasi_accuracies = []\n",
    "for genes_no in range(1, 9):\n",
    "    barabasi_acc = learn(BarabasiRegression(input_dim, genes_no, output_dim))\n",
    "    barabasi_accuracies.append(barabasi_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Line2D' object has no property 'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c5e55102faa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlogistic_acc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'logistic_regression'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbarabasi_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'barabasi_regression'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-workspace/pytorch_venv/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2824\u001b[0m     return gca().plot(\n\u001b[1;32m   2825\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2826\u001b[0;31m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-workspace/pytorch_venv/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1741\u001b[0m         \"\"\"\n\u001b[1;32m   1742\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-workspace/pytorch_venv/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-workspace/pytorch_venv/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"x has {ncx} columns but y has {ncy} columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         return [func(x[:, j % ncx], y[:, j % ncy], kw, kwargs)\n\u001b[0;32m--> 419\u001b[0;31m                 for j in range(max(ncx, ncy))]\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-workspace/pytorch_venv/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"x has {ncx} columns but y has {ncy} columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         return [func(x[:, j % ncx], y[:, j % ncy], kw, kwargs)\n\u001b[0;32m--> 419\u001b[0;31m                 for j in range(max(ncx, ncy))]\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-workspace/pytorch_venv/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_makeline\u001b[0;34m(self, x, y, kw, kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mdefault_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getdefaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setdefaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0mseg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-workspace/pytorch_venv/lib/python3.6/site-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, xdata, ydata, linewidth, linestyle, color, marker, markersize, markeredgewidth, markeredgecolor, markerfacecolor, markerfacecoloralt, fillstyle, antialiased, dash_capstyle, solid_capstyle, dash_joinstyle, solid_joinstyle, pickradius, drawstyle, markevery, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;31m# update kwargs before updating data to give the caller a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;31m# chance to init axes (and hence unit support)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickradius\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickradius\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mind_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-workspace/pytorch_venv/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, props)\u001b[0m\n\u001b[1;32m    994\u001b[0m                     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"set_{k}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m                         raise AttributeError(f\"{type(self).__name__!r} object \"\n\u001b[0m\u001b[1;32m    997\u001b[0m                                              f\"has no property {k!r}\")\n\u001b[1;32m    998\u001b[0m                     \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Line2D' object has no property 'title'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(np.arange(1, 9), [logistic_acc]*8, label='logistic_regression')\n",
    "plt.plot(np.arange(1, 9), barabasi_accuracies, label='barabasi_regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
